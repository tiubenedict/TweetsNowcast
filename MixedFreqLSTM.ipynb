{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2.6.0+cu124\n",
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x796541d1c2e0>\n",
      "Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # ReLU, Softmax\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset #, IterableDataset, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from NowcastingPipelineM import NowcastingPH_M\n",
    "import dynamicfactoranalysis as dfa\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # torch.backends.cudnn.deterministic = True                     # Only cuDNN convolution algorithms\n",
    "    torch.use_deterministic_algorithms(True)                        # All torch and cuDNN algorithms when available. RunTime error if not available.\n",
    "    torch.backends.cudnn.benchmark = False                          # Uses the same algorthim. May lose out on performance.\n",
    "    # torch.utils.deterministic.fill_uninitialized_memory = True    # For torch.empty() or torch.Tensor.resize()\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'                 # For RNN/LSTM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NowcastingLSTM_MQ(NowcastingPH_M):\n",
    "    def set_classname(self, **kwargs):\n",
    "        self.prefix = f'LSTM{self.kwargs.get(\"lag_order\")} x ' + ('DFM_Opt' if self.kwargs.get(\"optimize_order\") else f'DFM{self.kwargs.get(\"DFM_order\")}') if self.kwargs.get(\"extend\") else f'LSTM{self.kwargs.get(\"lag_order\")}'\n",
    "    def load_tweets(self, vintage, window, kmpair, freq='M', extend=False, **kwargs):\n",
    "        vintage = pd.to_datetime(vintage)\n",
    "        tweets = pd.read_csv('data/PH_Tweets_v3.csv')\n",
    "        tweets['date'] = pd.to_datetime(tweets['date']) + pd.offsets.MonthEnd(0)\n",
    "        tweets = tweets.set_index('date')\n",
    "\n",
    "        if len(kmpair) == 0:\n",
    "            kmpair = {keyword: list(tweets.columns.drop('keyword')) for keyword in tweets['keyword'].unique()}\n",
    "        data = [tweets[tweets['keyword'] == keyword][kmpair[keyword]].add_suffix(f'_{keyword}') for keyword in kmpair.keys()]\n",
    "        tweets = reduce(lambda left, right: pd.merge(left, right, on='date', how='outer', sort=True), data)\n",
    "        tweets = tweets.loc[dt.datetime(2010,1,1) : pd.to_datetime(vintage), :]\n",
    "        # tweets = tweets.loc[pd.to_datetime(vintage)  - relativedelta(months =  (pd.to_datetime(vintage).month - 1)%3 + window) : pd.to_datetime(vintage), :]\n",
    "        # tweets = super().load_tweets(vintage, freq='M', **kwargs)\n",
    "        # DFM_order = self.kwargs.get('DFM_order')                                             ### temporary measure to solve stationarity error\n",
    "        # kwargs['DFM_order'] = (1, DFM_order[1], DFM_order[2], DFM_order[3])                   ### temporary measure to solve stationarity error\n",
    "        tweets = self.extend_data(tweets, vintage, **kwargs) if extend else tweets\n",
    "        tweets.index = pd.PeriodIndex(tweets.index, freq=freq)\n",
    "        return tweets\n",
    "    def extend_data(self, df, vintage, DFM_order, optimize_order=False, **kwargs):\n",
    "        ### Instead of extending until year end, just extend until current vintage\n",
    "        factor_order, error_order, k_factors, factor_lag = DFM_order\n",
    "        # drop row if not enough non-missing (max safety)\n",
    "        df = df.dropna(thresh = k_factors * (1 + factor_lag))\n",
    "\n",
    "        if optimize_order:\n",
    "            model = dfa.DynamicFactorModelOptimizer(\n",
    "                endog=df, k_factors_max=k_factors, factor_lag_max=factor_lag, factor_order_max=factor_order, \n",
    "                error_order_max=error_order, verbose=True,**kwargs).fit(**kwargs)\n",
    "        else:\n",
    "            model = dfa.DynamicFactorModel(\n",
    "                endog=df, k_factors=k_factors, factor_lag=factor_lag, factor_order=factor_order, \n",
    "                error_order=error_order, **kwargs)\n",
    "        results = model.fit(disp=False, maxiter=10, method='powell', ftol=1e-3, **kwargs)\n",
    "        # results = model.fit(disp=False, maxiter=1000, method='powell', ftol=1e-5, **kwargs)\n",
    "        \n",
    "        df_extended = pd.DataFrame()\n",
    "        for col in df.columns:\n",
    "            col_extended = pd.concat([df[[col]].dropna(), \n",
    "                                    results.predict(start=df[col].dropna().index[-1], end=vintage)[[col]].iloc[1:]])\n",
    "            df_extended = pd.concat([df_extended, col_extended], axis=1)\n",
    "        df_extended.index.name = df.index.name\n",
    "\n",
    "        return df_extended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ECN.pseiclose_YoY</th>\n",
       "      <th>ECN.tbill_usd90</th>\n",
       "      <th>ECN.phpusd_YoY</th>\n",
       "      <th>ECN.tdr_php360</th>\n",
       "      <th>ECN.govtexpt_YoY</th>\n",
       "      <th>ECN.m1_YoY</th>\n",
       "      <th>ECN.IPI_YoY</th>\n",
       "      <th>ECN.imports_YoY</th>\n",
       "      <th>ECN.exports_YoY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01</th>\n",
       "      <td>1.357160</td>\n",
       "      <td>2.546622</td>\n",
       "      <td>-0.321012</td>\n",
       "      <td>-0.573727</td>\n",
       "      <td>1.246838</td>\n",
       "      <td>0.367732</td>\n",
       "      <td>0.475311</td>\n",
       "      <td>2.979768</td>\n",
       "      <td>1.602402</td>\n",
       "      <td>2.201924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02</th>\n",
       "      <td>1.357160</td>\n",
       "      <td>2.590565</td>\n",
       "      <td>0.039815</td>\n",
       "      <td>-0.610389</td>\n",
       "      <td>1.325077</td>\n",
       "      <td>-0.503924</td>\n",
       "      <td>1.288223</td>\n",
       "      <td>2.606936</td>\n",
       "      <td>1.288057</td>\n",
       "      <td>2.202541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03</th>\n",
       "      <td>1.357160</td>\n",
       "      <td>2.395650</td>\n",
       "      <td>0.256312</td>\n",
       "      <td>-1.218834</td>\n",
       "      <td>1.340969</td>\n",
       "      <td>1.809788</td>\n",
       "      <td>2.249231</td>\n",
       "      <td>1.844254</td>\n",
       "      <td>2.177334</td>\n",
       "      <td>2.291599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04</th>\n",
       "      <td>1.243454</td>\n",
       "      <td>2.235894</td>\n",
       "      <td>0.256312</td>\n",
       "      <td>-1.601437</td>\n",
       "      <td>1.092804</td>\n",
       "      <td>0.932956</td>\n",
       "      <td>2.944650</td>\n",
       "      <td>2.429373</td>\n",
       "      <td>2.950015</td>\n",
       "      <td>1.345291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05</th>\n",
       "      <td>1.243454</td>\n",
       "      <td>1.117557</td>\n",
       "      <td>0.256312</td>\n",
       "      <td>-0.897047</td>\n",
       "      <td>1.169821</td>\n",
       "      <td>1.344380</td>\n",
       "      <td>1.642571</td>\n",
       "      <td>1.916994</td>\n",
       "      <td>1.696690</td>\n",
       "      <td>1.882029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08</th>\n",
       "      <td>0.739029</td>\n",
       "      <td>-0.451752</td>\n",
       "      <td>1.483126</td>\n",
       "      <td>0.188067</td>\n",
       "      <td>-0.745817</td>\n",
       "      <td>-0.072413</td>\n",
       "      <td>0.438996</td>\n",
       "      <td>0.715950</td>\n",
       "      <td>0.394573</td>\n",
       "      <td>-0.578610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09</th>\n",
       "      <td>0.739029</td>\n",
       "      <td>-0.395784</td>\n",
       "      <td>1.122298</td>\n",
       "      <td>0.247170</td>\n",
       "      <td>-0.813054</td>\n",
       "      <td>1.441119</td>\n",
       "      <td>-0.273999</td>\n",
       "      <td>0.502828</td>\n",
       "      <td>0.545799</td>\n",
       "      <td>0.043630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10</th>\n",
       "      <td>0.413168</td>\n",
       "      <td>-0.791660</td>\n",
       "      <td>1.555291</td>\n",
       "      <td>0.836004</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>-1.544243</td>\n",
       "      <td>-0.280197</td>\n",
       "      <td>0.362225</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>0.144558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11</th>\n",
       "      <td>0.413168</td>\n",
       "      <td>-1.131016</td>\n",
       "      <td>2.565608</td>\n",
       "      <td>0.894927</td>\n",
       "      <td>-0.627236</td>\n",
       "      <td>1.417577</td>\n",
       "      <td>-1.012744</td>\n",
       "      <td>1.103489</td>\n",
       "      <td>0.778714</td>\n",
       "      <td>-0.749637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12</th>\n",
       "      <td>0.413168</td>\n",
       "      <td>-1.102085</td>\n",
       "      <td>2.709939</td>\n",
       "      <td>1.082663</td>\n",
       "      <td>-0.503764</td>\n",
       "      <td>0.270953</td>\n",
       "      <td>-1.206215</td>\n",
       "      <td>2.037834</td>\n",
       "      <td>0.623287</td>\n",
       "      <td>-0.053784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           target  ECN.pseiclose_YoY  ECN.tbill_usd90  ECN.phpusd_YoY  \\\n",
       "date                                                                    \n",
       "2010-01  1.357160           2.546622        -0.321012       -0.573727   \n",
       "2010-02  1.357160           2.590565         0.039815       -0.610389   \n",
       "2010-03  1.357160           2.395650         0.256312       -1.218834   \n",
       "2010-04  1.243454           2.235894         0.256312       -1.601437   \n",
       "2010-05  1.243454           1.117557         0.256312       -0.897047   \n",
       "...           ...                ...              ...             ...   \n",
       "2016-08  0.739029          -0.451752         1.483126        0.188067   \n",
       "2016-09  0.739029          -0.395784         1.122298        0.247170   \n",
       "2016-10  0.413168          -0.791660         1.555291        0.836004   \n",
       "2016-11  0.413168          -1.131016         2.565608        0.894927   \n",
       "2016-12  0.413168          -1.102085         2.709939        1.082663   \n",
       "\n",
       "         ECN.tdr_php360  ECN.govtexpt_YoY  ECN.m1_YoY  ECN.IPI_YoY  \\\n",
       "date                                                                 \n",
       "2010-01        1.246838          0.367732    0.475311     2.979768   \n",
       "2010-02        1.325077         -0.503924    1.288223     2.606936   \n",
       "2010-03        1.340969          1.809788    2.249231     1.844254   \n",
       "2010-04        1.092804          0.932956    2.944650     2.429373   \n",
       "2010-05        1.169821          1.344380    1.642571     1.916994   \n",
       "...                 ...               ...         ...          ...   \n",
       "2016-08       -0.745817         -0.072413    0.438996     0.715950   \n",
       "2016-09       -0.813054          1.441119   -0.273999     0.502828   \n",
       "2016-10       -0.732369         -1.544243   -0.280197     0.362225   \n",
       "2016-11       -0.627236          1.417577   -1.012744     1.103489   \n",
       "2016-12       -0.503764          0.270953   -1.206215     2.037834   \n",
       "\n",
       "         ECN.imports_YoY  ECN.exports_YoY  \n",
       "date                                       \n",
       "2010-01         1.602402         2.201924  \n",
       "2010-02         1.288057         2.202541  \n",
       "2010-03         2.177334         2.291599  \n",
       "2010-04         2.950015         1.345291  \n",
       "2010-05         1.696690         1.882029  \n",
       "...                  ...              ...  \n",
       "2016-08         0.394573        -0.578610  \n",
       "2016-09         0.545799         0.043630  \n",
       "2016-10         0.018322         0.144558  \n",
       "2016-11         0.778714        -0.749637  \n",
       "2016-12         0.623287        -0.053784  \n",
       "\n",
       "[84 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = 'GDP'\n",
    "kmpair = {'PE': ['CRVADER_BVN','CR_BxP_0'],'PU+': ['CRVADER_BVN','CR_BxP_0']} # kmpair = {'PE':['CR_B0']}\n",
    "window = 1000\n",
    "extend = False\n",
    "DFM_order = (1,0,1,0)\n",
    "model = NowcastingLSTM_MQ(extend = extend, DFM_order=DFM_order, optimize_order = False, kmpair=kmpair, target=target)\n",
    "data, target_scaler, econ_scaler, tweets_scaler = model.load_data(vintage=pd.to_datetime('2017-03-31'),window=window, kmpair=kmpair, with_econ=True, with_tweets=False, target_release_lag=True,scaled=True)\n",
    "data = data.loc[pd.to_datetime('2010-01-31'):,:].dropna()#.reset_index()\n",
    "# data[:] = StandardScaler().fit_transform(data)\n",
    "# model.load_target(vintage=pd.to_datetime('2017-01-31'), target='GDP', growth=True, quarterly=True, freq='M', target_release_lag=False)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ECN.pseiclose_YoY</th>\n",
       "      <th>ECN.tbill_usd90</th>\n",
       "      <th>ECN.phpusd_YoY</th>\n",
       "      <th>ECN.tdr_php360</th>\n",
       "      <th>ECN.govtexpt_YoY</th>\n",
       "      <th>ECN.m1_YoY</th>\n",
       "      <th>ECN.IPI_YoY</th>\n",
       "      <th>ECN.imports_YoY</th>\n",
       "      <th>ECN.exports_YoY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01</th>\n",
       "      <td>1.357160</td>\n",
       "      <td>2.546622</td>\n",
       "      <td>-0.321012</td>\n",
       "      <td>-0.573727</td>\n",
       "      <td>1.246838</td>\n",
       "      <td>0.367732</td>\n",
       "      <td>0.475311</td>\n",
       "      <td>2.979768</td>\n",
       "      <td>1.602402</td>\n",
       "      <td>2.201924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02</th>\n",
       "      <td>1.357160</td>\n",
       "      <td>2.590565</td>\n",
       "      <td>0.039815</td>\n",
       "      <td>-0.610389</td>\n",
       "      <td>1.325077</td>\n",
       "      <td>-0.503924</td>\n",
       "      <td>1.288223</td>\n",
       "      <td>2.606936</td>\n",
       "      <td>1.288057</td>\n",
       "      <td>2.202541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03</th>\n",
       "      <td>1.357160</td>\n",
       "      <td>2.395650</td>\n",
       "      <td>0.256312</td>\n",
       "      <td>-1.218834</td>\n",
       "      <td>1.340969</td>\n",
       "      <td>1.809788</td>\n",
       "      <td>2.249231</td>\n",
       "      <td>1.844254</td>\n",
       "      <td>2.177334</td>\n",
       "      <td>2.291599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04</th>\n",
       "      <td>1.243454</td>\n",
       "      <td>2.235894</td>\n",
       "      <td>0.256312</td>\n",
       "      <td>-1.601437</td>\n",
       "      <td>1.092804</td>\n",
       "      <td>0.932956</td>\n",
       "      <td>2.944650</td>\n",
       "      <td>2.429373</td>\n",
       "      <td>2.950015</td>\n",
       "      <td>1.345291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05</th>\n",
       "      <td>1.243454</td>\n",
       "      <td>1.117557</td>\n",
       "      <td>0.256312</td>\n",
       "      <td>-0.897047</td>\n",
       "      <td>1.169821</td>\n",
       "      <td>1.344380</td>\n",
       "      <td>1.642571</td>\n",
       "      <td>1.916994</td>\n",
       "      <td>1.696690</td>\n",
       "      <td>1.882029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08</th>\n",
       "      <td>0.739029</td>\n",
       "      <td>-0.451752</td>\n",
       "      <td>1.483126</td>\n",
       "      <td>0.188067</td>\n",
       "      <td>-0.745817</td>\n",
       "      <td>-0.072413</td>\n",
       "      <td>0.438996</td>\n",
       "      <td>0.715950</td>\n",
       "      <td>0.394573</td>\n",
       "      <td>-0.578610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09</th>\n",
       "      <td>0.739029</td>\n",
       "      <td>-0.395784</td>\n",
       "      <td>1.122298</td>\n",
       "      <td>0.247170</td>\n",
       "      <td>-0.813054</td>\n",
       "      <td>1.441119</td>\n",
       "      <td>-0.273999</td>\n",
       "      <td>0.502828</td>\n",
       "      <td>0.545799</td>\n",
       "      <td>0.043630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10</th>\n",
       "      <td>0.413168</td>\n",
       "      <td>-0.791660</td>\n",
       "      <td>1.555291</td>\n",
       "      <td>0.836004</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>-1.544243</td>\n",
       "      <td>-0.280197</td>\n",
       "      <td>0.362225</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>0.144558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11</th>\n",
       "      <td>0.413168</td>\n",
       "      <td>-1.131016</td>\n",
       "      <td>2.565608</td>\n",
       "      <td>0.894927</td>\n",
       "      <td>-0.627236</td>\n",
       "      <td>1.417577</td>\n",
       "      <td>-1.012744</td>\n",
       "      <td>1.103489</td>\n",
       "      <td>0.778714</td>\n",
       "      <td>-0.749637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12</th>\n",
       "      <td>0.413168</td>\n",
       "      <td>-1.102085</td>\n",
       "      <td>2.709939</td>\n",
       "      <td>1.082663</td>\n",
       "      <td>-0.503764</td>\n",
       "      <td>0.270953</td>\n",
       "      <td>-1.206215</td>\n",
       "      <td>2.037834</td>\n",
       "      <td>0.623287</td>\n",
       "      <td>-0.053784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           target  ECN.pseiclose_YoY  ECN.tbill_usd90  ECN.phpusd_YoY  \\\n",
       "date                                                                    \n",
       "2010-01  1.357160           2.546622        -0.321012       -0.573727   \n",
       "2010-02  1.357160           2.590565         0.039815       -0.610389   \n",
       "2010-03  1.357160           2.395650         0.256312       -1.218834   \n",
       "2010-04  1.243454           2.235894         0.256312       -1.601437   \n",
       "2010-05  1.243454           1.117557         0.256312       -0.897047   \n",
       "...           ...                ...              ...             ...   \n",
       "2016-08  0.739029          -0.451752         1.483126        0.188067   \n",
       "2016-09  0.739029          -0.395784         1.122298        0.247170   \n",
       "2016-10  0.413168          -0.791660         1.555291        0.836004   \n",
       "2016-11  0.413168          -1.131016         2.565608        0.894927   \n",
       "2016-12  0.413168          -1.102085         2.709939        1.082663   \n",
       "\n",
       "         ECN.tdr_php360  ECN.govtexpt_YoY  ECN.m1_YoY  ECN.IPI_YoY  \\\n",
       "date                                                                 \n",
       "2010-01        1.246838          0.367732    0.475311     2.979768   \n",
       "2010-02        1.325077         -0.503924    1.288223     2.606936   \n",
       "2010-03        1.340969          1.809788    2.249231     1.844254   \n",
       "2010-04        1.092804          0.932956    2.944650     2.429373   \n",
       "2010-05        1.169821          1.344380    1.642571     1.916994   \n",
       "...                 ...               ...         ...          ...   \n",
       "2016-08       -0.745817         -0.072413    0.438996     0.715950   \n",
       "2016-09       -0.813054          1.441119   -0.273999     0.502828   \n",
       "2016-10       -0.732369         -1.544243   -0.280197     0.362225   \n",
       "2016-11       -0.627236          1.417577   -1.012744     1.103489   \n",
       "2016-12       -0.503764          0.270953   -1.206215     2.037834   \n",
       "\n",
       "         ECN.imports_YoY  ECN.exports_YoY  \n",
       "date                                       \n",
       "2010-01         1.602402         2.201924  \n",
       "2010-02         1.288057         2.202541  \n",
       "2010-03         2.177334         2.291599  \n",
       "2010-04         2.950015         1.345291  \n",
       "2010-05         1.696690         1.882029  \n",
       "...                  ...              ...  \n",
       "2016-08         0.394573        -0.578610  \n",
       "2016-09         0.545799         0.043630  \n",
       "2016-10         0.018322         0.144558  \n",
       "2016-11         0.778714        -0.749637  \n",
       "2016-12         0.623287        -0.053784  \n",
       "\n",
       "[84 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 15, 9)\n",
      "(24, 4, 1)\n",
      "(24, 1)\n"
     ]
    }
   ],
   "source": [
    "def sliding_windows(data_x, seq_length, freq_ratio = 3):\n",
    "    '''\n",
    "    seq_length = number of rows of historical data + number of rows of input data (i.e. 12 + 3)\n",
    "    '''\n",
    "    x_encoder_in = []\n",
    "    y_decoder_in = []\n",
    "    y_target = []\n",
    "\n",
    "    for i in range(0,len(data_x)-seq_length+1, freq_ratio):\n",
    "        _x_in = data.iloc[i:(i+seq_length),1:]\n",
    "        _y_in = data_x.iloc[i+freq_ratio-1:i+seq_length-freq_ratio:freq_ratio, :1] # gets every rth row but stops before the current low-freq vintage\n",
    "        _y_out = data_x.iloc[i+seq_length-1, :1]\n",
    "        x_encoder_in.append(_x_in)\n",
    "        y_decoder_in.append(_y_in)\n",
    "        y_target.append(_y_out)\n",
    "    \n",
    "    return np.array(x_encoder_in),np.array(y_decoder_in), np.array(y_target)\n",
    "x_encoder_in, y_decoder_in, y_target = sliding_windows(data, seq_length=15)\n",
    "print(x_encoder_in.shape) # (timesteps, seq_length, dim_x)\n",
    "print(y_decoder_in.shape) # (timesteps, dim_y)\n",
    "print(y_target.shape)\n",
    "trainX_in = Variable(torch.Tensor(x_encoder_in))\n",
    "trainY_in = Variable(torch.Tensor(y_decoder_in))\n",
    "trainY_out = Variable(torch.Tensor(y_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### seq2seq y\n",
    "class PreAttnEncoder(nn.Module):\n",
    "    def __init__(self, dim_x, n_a, dropout_rate=0.2, bidirectional_encoder=False):\n",
    "        super(PreAttnEncoder, self).__init__()\n",
    "        self.bidirectional_encoder = bidirectional_encoder\n",
    "        self.lstm = nn.LSTM(dim_x, n_a, batch_first=True, bidirectional=bidirectional_encoder)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a, _ = self.lstm(x)\n",
    "        a = self.dropout(a)\n",
    "        return a\n",
    "\n",
    "class OneStepAttn(nn.Module):\n",
    "    def __init__(self, n_a, n_s, n_align):\n",
    "        super(OneStepAttn, self).__init__()\n",
    "        self.densor1 = nn.Linear(n_a + n_s, n_align)\n",
    "        self.densor2 = nn.Linear(n_align, 1)\n",
    "\n",
    "    def forward(self, a, s_prev):\n",
    "        s_prev = s_prev.unsqueeze(1).repeat(1, a.size(1), 1) # (batch_size, Lx, n_s)\n",
    "        concat = torch.cat((a, s_prev), dim=-1) # (batch_size, Lx, n_a + n_s)\n",
    "        e = torch.tanh(self.densor1(concat))\n",
    "        energies = F.relu(self.densor2(e))  # (batch_size, Lx, 1)\n",
    "        alphas = F.softmax(energies, dim=1) # (batch_size, Lx, 1)\n",
    "        context = torch.bmm(alphas.transpose(1, 2), a).squeeze(1) # (batch_size, n_a)\n",
    "        return context\n",
    "\n",
    "class MTMFSeq2Seq(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, Lx, Ty, n_a, n_s, n_align_y, fc_y, dropout_rate, freq_ratio=3, bidirectional_encoder=False, l1reg=1e-5, l2reg=1e-4):\n",
    "        super(MTMFSeq2Seq, self).__init__()\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_y = dim_y\n",
    "        self.Lx = Lx\n",
    "        self.Ty = Ty\n",
    "        self.n_a = n_a\n",
    "        self.n_s = n_s\n",
    "        self.n_align_y = n_align_y\n",
    "        self.fc_y = fc_y\n",
    "        self.freq_ratio = freq_ratio\n",
    "        self.bidirectional_encoder = bidirectional_encoder\n",
    "        self.l1reg = l1reg\n",
    "        self.l2reg = l2reg\n",
    "\n",
    "        self.pre_attn = PreAttnEncoder(dim_x, n_a, dropout_rate, bidirectional_encoder)\n",
    "        self.one_step_attention_y = OneStepAttn(n_a, n_s, n_align_y)\n",
    "\n",
    "        self.post_attn_y = nn.LSTMCell(n_a + dim_y, n_s)\n",
    "        self.ffn1_y = nn.Linear(n_s, fc_y)\n",
    "        self.dropout_fn_y = nn.Dropout(dropout_rate)\n",
    "        self.ffn2_y = nn.Linear(fc_y, dim_y) ### add regularizer\n",
    "\n",
    "    def initialize_state(self, batch_size, dim, device):\n",
    "        return torch.zeros(batch_size, dim, device=device)\n",
    "\n",
    "    def forward(self, x_encoder_in, y_decoder_in):\n",
    "        batch_size = x_encoder_in.size(0)\n",
    "\n",
    "        a = self.pre_attn(x_encoder_in)\n",
    "\n",
    "        s_y, c_y = self.initialize_state(batch_size, self.n_s, device), self.initialize_state(batch_size, self.n_s, device)\n",
    "        for t in range(self.Ty):\n",
    "            a_idx = int((t + 1) * self.freq_ratio - 1)\n",
    "            a_to_attend = a[:, (a_idx - self.freq_ratio + 1):(a_idx + 1), :]\n",
    "            context = self.one_step_attention_y(a_to_attend, s_y)\n",
    "            post_attn_input = torch.cat((context, y_decoder_in[:, t, :]), dim=-1)\n",
    "            s_y, c_y = self.post_attn_y(post_attn_input, (s_y, c_y))\n",
    "\n",
    "        y_pred = self.ffn1_y(s_y)\n",
    "        y_pred = self.dropout_fn_y(y_pred)\n",
    "        y_pred = self.ffn2_y(y_pred)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type        | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model     | MTMFSeq2Seq | 506 K  | train\n",
      "1 | criterion | MSELoss     | 0      | train\n",
      "--------------------------------------------------\n",
      "506 K     Trainable params\n",
      "0         Non-trainable params\n",
      "506 K     Total params\n",
      "2.027     Total estimated model params size (MB)\n",
      "12        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 12/12 [00:00<00:00, 24.64it/s, v_num=30, train_loss=0.0272]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 12/12 [00:00<00:00, 22.70it/s, v_num=30, train_loss=0.0272]\n"
     ]
    }
   ],
   "source": [
    "class MTMFSeq2SeqLightning(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=0.001):\n",
    "        super(MTMFSeq2SeqLightning, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x_encoder_in, y_decoder_in):\n",
    "        return self.model(x_encoder_in, y_decoder_in)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_encoder_in, y_decoder_in, y_target = batch\n",
    "        y_pred = self(x_encoder_in, y_decoder_in)\n",
    "        loss = self.criterion(y_pred, y_target)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# Prepare the dataset and dataloader\n",
    "train_dataset = TensorDataset(trainX_in, trainY_in, trainY_out)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=8)\n",
    "\n",
    "# Initialize the model\n",
    "dim_x = trainX_in.shape[-1] # 9\n",
    "dim_y = trainY_in.shape[-1] # 1\n",
    "Lx = trainX_in.shape[1] # 15\n",
    "Ty = trainY_in.shape[1] # 4\n",
    "n_a = 128\n",
    "n_s = 256\n",
    "n_align_y = 16\n",
    "fc_y = 128\n",
    "dropout_rate = 0.4\n",
    "freq_ratio = 3\n",
    "bidirectional_encoder = False\n",
    "\n",
    "model = MTMFSeq2Seq(\n",
    "    dim_x=dim_x,\n",
    "    dim_y=dim_y,\n",
    "    Lx=Lx,\n",
    "    Ty=Ty,\n",
    "    n_a=n_a,\n",
    "    n_s=n_s,\n",
    "    n_align_y=n_align_y,\n",
    "    fc_y=fc_y,\n",
    "    dropout_rate=dropout_rate,\n",
    "    freq_ratio=freq_ratio,\n",
    "    bidirectional_encoder=bidirectional_encoder,\n",
    ")\n",
    "\n",
    "# Initialize the Lightning module\n",
    "lightning_model = MTMFSeq2SeqLightning(model)\n",
    "\n",
    "# Train the model\n",
    "trainer = pl.Trainer(max_epochs=100, accelerator='auto',log_every_n_steps=2)\n",
    "trainer.fit(lightning_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1a907235e552bf44\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1a907235e552bf44\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### seq2one y\n",
    "# class PreAttnEncoder(nn.Module):\n",
    "#     \"\"\"Pre-attention Encoder module\"\"\"\n",
    "#     def __init__(self, dim_x, n_a, dropout_rate=0.2, bidirectional_encoder=False):\n",
    "#         super(PreAttnEncoder, self).__init__()\n",
    "#         self.bidirectional_encoder = bidirectional_encoder\n",
    "#         self.lstm = nn.LSTM( input_size=dim_x, hidden_size=n_a, batch_first=True, bidirectional=bidirectional_encoder)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         a, _ = self.lstm(x)\n",
    "#         a = self.dropout(a)\n",
    "#         return a\n",
    "\n",
    "\n",
    "# class OneStepAttn(nn.Module):\n",
    "#     \"\"\"Attention alignment module\"\"\"\n",
    "#     def __init__(self, n_a, n_s, n_align):\n",
    "#         super(OneStepAttn, self).__init__()\n",
    "#         self.densor1 = nn.Linear(n_a + n_s, n_align)\n",
    "#         self.densor2 = nn.Linear(n_align, 1)\n",
    "\n",
    "#     def forward(self, a, s_prev):\n",
    "#         s_prev = s_prev.unsqueeze(1).repeat(1, a.size(1), 1)\n",
    "#         concat = torch.cat((a, s_prev), dim=-1)\n",
    "#         e = torch.tanh(self.densor1(concat))\n",
    "#         energies = F.relu(self.densor2(e))\n",
    "#         alphas = F.softmax(energies, dim=1)\n",
    "#         context = torch.sum(alphas * a, dim=1)\n",
    "#         return context\n",
    "\n",
    "\n",
    "# class MTMFSeq2One(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         Lx,\n",
    "#         dim_x,\n",
    "#         Ty,\n",
    "#         dim_y,\n",
    "#         n_a,\n",
    "#         n_s,\n",
    "#         n_align,\n",
    "#         fc_y,\n",
    "#         dropout_rate,\n",
    "#         freq_ratio=3,\n",
    "#         bidirectional_encoder=False\n",
    "#     ):\n",
    "#         super(MTMFSeq2One, self).__init__()\n",
    "#         self.Lx = Lx\n",
    "#         self.Ty = Ty\n",
    "#         self.dim_x = dim_x\n",
    "#         self.dim_y = dim_y\n",
    "#         self.n_a = n_a\n",
    "#         self.n_s = n_s\n",
    "#         self.n_align = n_align\n",
    "#         self.fc_y = fc_y\n",
    "#         self.freq_ratio = freq_ratio\n",
    "#         self.bidirectional_encoder = bidirectional_encoder\n",
    "\n",
    "#         # Encoder\n",
    "#         self.pre_attn = PreAttnEncoder(dim_x, n_a, dropout_rate, bidirectional_encoder)\n",
    "\n",
    "#         # Attention alignment model\n",
    "#         self.one_step_attention = OneStepAttn(n_a, n_s, n_align)\n",
    "\n",
    "#         # Decoder\n",
    "#         self.post_attn = nn.LSTMCell(input_size=n_a + dim_y, hidden_size=n_s)\n",
    "#         self.ffn1 = nn.Linear(n_s, fc_y)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.ffn2 = nn.Linear(fc_y, dim_y) ### add regularizer\n",
    "\n",
    "#     def initialize_state(self, batch_size, dim):\n",
    "#         return torch.zeros(batch_size, dim)\n",
    "\n",
    "#     def forward(self, batch_inputs):\n",
    "#         x, y = batch_inputs\n",
    "#         batch_size = x.size(0)\n",
    "\n",
    "#         # Stage 1: Pre-attention encoding\n",
    "#         a = self.pre_attn(x)\n",
    "\n",
    "#         # Stage 2: Attention-based decoding\n",
    "#         s = self.initialize_state(batch_size, self.n_s).to(x.device)\n",
    "#         c = self.initialize_state(batch_size, self.n_s).to(x.device)\n",
    "\n",
    "#         for t in range(self.Ty):\n",
    "#             a_idx = int((t + 1) * self.freq_ratio - 1)\n",
    "#             a_to_attend = a[:, (a_idx - self.freq_ratio + 1):(a_idx + 1), :]\n",
    "#             context = self.one_step_attention(a_to_attend, s)\n",
    "\n",
    "#             post_attn_input = torch.cat((context, y[:, t, :].unsqueeze(1)), dim=-1)\n",
    "#             s, c = self.post_attn(post_attn_input.squeeze(1), (s, c))\n",
    "\n",
    "#         y_pred = F.relu(self.ffn1(s))\n",
    "#         y_pred = self.dropout(y_pred)\n",
    "#         y_pred = self.ffn2(y_pred)\n",
    "\n",
    "#         return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### seq2one x, y\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class PreAttnEncoder(nn.Module):\n",
    "#     \"\"\"Pre-attention Encoder module\"\"\"\n",
    "#     def __init__(self, dim_x, fc_dim, n_a, dropout_rate=0.2, bidirectional_encoder=False, l1reg=1e-5, l2reg=1e-4):\n",
    "#         super(PreAttnEncoder, self).__init__()\n",
    "#         self.bidirectional_encoder = bidirectional_encoder\n",
    "#         self.l1reg = l1reg\n",
    "#         self.l2reg = l2reg\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_size=dim_x, hidden_size=n_a, batch_first=True, bidirectional=bidirectional_encoder)\n",
    "#         self.ffn1 = nn.Linear(n_a * (2 if bidirectional_encoder else 1), fc_dim)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.ffn2 = nn.Linear(fc_dim, dim_x) ### add regularizer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass for the encoder\n",
    "#         Args:\n",
    "#             x: (tensor) shape (batch_size, Lx, dim_x)\n",
    "#         Returns:\n",
    "#             x_pred: (tensor), the next step prediction for x (batch_size, dim_x)\n",
    "#             a: (tensor) sequence of LSTM hidden states, (batch_size, Lx, n_a)\n",
    "#         \"\"\"\n",
    "#         a, _ = self.lstm(x)\n",
    "#         x_pred = F.relu(self.ffn1(a[:, -1, :]))\n",
    "#         x_pred = self.dropout(x_pred)\n",
    "#         x_pred = self.ffn2(x_pred)\n",
    "#         return x_pred, a\n",
    "\n",
    "\n",
    "# class OneStepAttn(nn.Module):\n",
    "#     \"\"\"Attention alignment module\"\"\"\n",
    "#     def __init__(self, n_a, n_s, n_align):\n",
    "#         super(OneStepAttn, self).__init__()\n",
    "#         self.densor1 = nn.Linear(n_a + n_s, n_align)\n",
    "#         self.densor2 = nn.Linear(n_align, 1)\n",
    "\n",
    "#     def forward(self, a, s_prev):\n",
    "#         \"\"\"\n",
    "#         Performs one step of attention\n",
    "#         Args:\n",
    "#             a: hidden state from the pre-attention LSTM, shape = (batch_size, Lx, n_a)\n",
    "#             s_prev: previous hidden state of the post-attention LSTM, shape = (batch_size, n_s)\n",
    "#         Returns:\n",
    "#             context: context vector, input of the next post-attention LSTM cell\n",
    "#         \"\"\"\n",
    "#         s_prev = s_prev.unsqueeze(1).repeat(1, a.size(1), 1)  # (batch_size, Lx, n_s)\n",
    "#         concat = torch.cat((a, s_prev), dim=-1)  # (batch_size, Lx, n_a + n_s)\n",
    "#         e = torch.tanh(self.densor1(concat))\n",
    "#         energies = F.relu(self.densor2(e))  # (batch_size, Lx, 1)\n",
    "#         alphas = F.softmax(energies, dim=1)  # (batch_size, Lx, 1)\n",
    "#         context = torch.bmm(alphas.transpose(1,2), a).squeeze(1)  # (batch_size, n_a)\n",
    "#         return context\n",
    "\n",
    "\n",
    "# class MTMFSeq2One(nn.Module):\n",
    "#     def __init__( self, Lx, dim_x, Ty, dim_y, n_a, n_s, n_align, fc_x, fc_y, dropout_rate, freq_ratio=3, bidirectional_encoder=False, l1reg=1e-5, l2reg=1e-4):\n",
    "#         super(MTMFSeq2One, self).__init__()\n",
    "#         self.dim_x = dim_x\n",
    "#         self.dim_y = dim_y\n",
    "#         self.Lx = Lx\n",
    "#         self.Ty = Ty\n",
    "#         self.n_a = n_a\n",
    "#         self.n_s = n_s\n",
    "#         self.n_align = n_align\n",
    "#         self.fc_x = fc_x\n",
    "#         self.fc_y = fc_y\n",
    "#         self.freq_ratio = freq_ratio\n",
    "#         self.bidirectional_encoder = bidirectional_encoder\n",
    "#         self.l1reg = l1reg\n",
    "#         self.l2reg = l2reg\n",
    "\n",
    "#         # Encoder\n",
    "#         self.pre_attn = PreAttnEncoder(dim_x, fc_x, n_a, dropout_rate, bidirectional_encoder, l1reg, l2reg)\n",
    "#         # Attention alignment model\n",
    "#         self.one_step_attention = OneStepAttn(n_a, n_s, n_align)\n",
    "#         # Decoder\n",
    "#         self.post_attn = nn.LSTMCell(n_a + dim_y, n_s)\n",
    "#         self.ffn1 = nn.Linear(n_s, fc_y)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.ffn2 = nn.Linear(fc_y, dim_y) ### add regularizer\n",
    "\n",
    "#     def initialize_state(self, batch_size, dim):\n",
    "#         return torch.zeros(batch_size, dim)\n",
    "\n",
    "#     def forward(self, batch_inputs):\n",
    "#         \"\"\"\n",
    "#         Forward pass\n",
    "#         Args:\n",
    "#             batch_inputs: tuple of (x, y) where\n",
    "#                 x: encoder input, shape (batch_size, Lx, dim_x)\n",
    "#                 y: decoder input, shape (batch_size, Ty, dim_y)\n",
    "#         Returns:\n",
    "#             x_pred: prediction for x\n",
    "#             y_pred: prediction for y\n",
    "#         \"\"\"\n",
    "#         x, y = batch_inputs\n",
    "#         batch_size = x.size(0)\n",
    "\n",
    "#         # Stage 1: Pre-attention encoding\n",
    "#         x_pred, a = self.pre_attn(x)\n",
    "\n",
    "#         # Stage 2: Attention-based decoding\n",
    "#         s = self.initialize_state(batch_size, self.n_s).to(x.device)\n",
    "#         c = self.initialize_state(batch_size, self.n_s).to(x.device)\n",
    "#         for t in range(self.Ty):\n",
    "#             a_idx = int((t + 1) * self.freq_ratio - 1)\n",
    "#             a_to_attend = a[:, (a_idx - self.freq_ratio + 1):(a_idx + 1), :]\n",
    "#             context = self.one_step_attention(a_to_attend, s)\n",
    "#             post_attn_input = torch.cat((context, y[:, t, :]), dim=-1)\n",
    "#             s, c = self.post_attn(post_attn_input, (s, c))\n",
    "\n",
    "#         y_pred = F.relu(self.ffn1(s))\n",
    "#         y_pred = self.dropout(y_pred)\n",
    "#         y_pred = self.ffn2(y_pred)\n",
    "#         return x_pred, y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
